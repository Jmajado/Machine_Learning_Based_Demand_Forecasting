{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "# --- Configuración Inicial ---\n",
    "print(\"Iniciando el pipeline simplificado para predicción de demanda con LightGBM...\")\n",
    "\n",
    "# Definir directorios relativos\n",
    "current_dir = os.getcwd()\n",
    "data_path = os.path.join(current_dir, \"../data/raw/Dataframe_Final_Data_LSTM.csv\")\n",
    "models_dir = os.path.join(current_dir, \"../data/models/\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"El archivo {data_path} no se encuentra.\")\n",
    "\n",
    "# Parámetros\n",
    "item_id = 'HOBBIES_1_001'\n",
    "store_id = 'CA_1'\n",
    "initial_seq_length = 3\n",
    "print(f\"Entrenando modelo para item_id: {item_id}, store_id: {store_id}\")\n",
    "\n",
    "# --- Paso 1: Cargar y Preparar los Datos ---\n",
    "df = pd.read_csv(data_path, parse_dates=['date'])\n",
    "df_filtered = df[(df['item_id'] == item_id) & (df['store_id'] == store_id)].copy()\n",
    "df_filtered = df_filtered.sort_values('date')\n",
    "\n",
    "# Verificar si hay datos después del filtrado\n",
    "if df_filtered.empty:\n",
    "    raise ValueError(f\"No hay datos para item_id: {item_id} y store_id: {store_id}. Verifica los filtros.\")\n",
    "print(f\"Número de filas para item_id={item_id} y store_id={store_id}: {len(df_filtered)}\")\n",
    "\n",
    "# Rellenar fechas faltantes\n",
    "date_range = pd.date_range(start=df_filtered['date'].min(), end=df_filtered['date'].max())\n",
    "missing_dates = date_range.difference(df_filtered['date'])\n",
    "if not missing_dates.empty:\n",
    "    print(f\"Faltan {len(missing_dates)} fechas. Rellenando con ventas = 0.\")\n",
    "    df_missing = pd.DataFrame({\n",
    "        'date': missing_dates,\n",
    "        'item_id': item_id,\n",
    "        'store_id': store_id,\n",
    "        'sales': 0,\n",
    "        'snap_CA': 0,\n",
    "        'sell_price': df_filtered['sell_price'].mean() if not df_filtered['sell_price'].isna().all() else 0,\n",
    "        'event_name_1': 'NoEvent',\n",
    "        'event_type_1': 'NoType',\n",
    "        'event_name_2': 'NoEvent',\n",
    "        'event_type_2': 'NoType',\n",
    "        'wm_yr_wk': 0,\n",
    "        'weekday': missing_dates.day_name(),\n",
    "        'wday': missing_dates.dayofweek + 1,\n",
    "        'month': missing_dates.month,\n",
    "        'year': missing_dates.year\n",
    "    })\n",
    "    df_filtered = pd.concat([df_filtered, df_missing]).sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Rellenar valores faltantes en columnas originales\n",
    "df_filtered['sell_price'] = df_filtered['sell_price'].fillna(df_filtered['sell_price'].mean())\n",
    "df_filtered['snap_CA'] = df_filtered['snap_CA'].fillna(0)\n",
    "print(f\"Número de filas después de rellenar valores faltantes: {len(df_filtered)}\")\n",
    "\n",
    "# Suavizar picos en las ventas (media móvil simple)\n",
    "df_filtered['sales_smoothed'] = df_filtered['sales'].rolling(window=7, min_periods=1, center=True).mean()\n",
    "df_filtered['sales'] = df_filtered['sales_smoothed'].fillna(df_filtered['sales'])\n",
    "\n",
    "# Transformar las ventas para suavizar la distribución\n",
    "df_filtered['sales'] = np.log1p(df_filtered['sales'])\n",
    "\n",
    "# Ingeniería de características avanzada\n",
    "df_filtered['lag_1'] = df_filtered['sales'].shift(1)\n",
    "df_filtered['lag_7'] = df_filtered['sales'].shift(7)\n",
    "df_filtered['lag_14'] = df_filtered['sales'].shift(14)\n",
    "df_filtered['rolling_mean_7'] = df_filtered['sales'].rolling(window=7).mean()\n",
    "df_filtered['rolling_std_7'] = df_filtered['sales'].rolling(window=7).std()\n",
    "df_filtered['rolling_min_7'] = df_filtered['sales'].rolling(window=7).min()\n",
    "df_filtered['rolling_max_7'] = df_filtered['sales'].rolling(window=7).max()\n",
    "df_filtered['day_of_week'] = df_filtered['date'].dt.dayofweek\n",
    "df_filtered['is_weekend'] = df_filtered['day_of_week'].isin([5, 6]).astype(int)\n",
    "df_filtered['month'] = df_filtered['date'].dt.month\n",
    "df_filtered['event'] = df_filtered['event_name_1'].notna().astype(int)\n",
    "df_filtered['snap'] = df_filtered['snap_CA']\n",
    "df_filtered['price_change'] = df_filtered['sell_price'].pct_change()\n",
    "df_filtered['week_of_year'] = df_filtered['date'].dt.isocalendar().week\n",
    "\n",
    "# Convertir week_of_year a tipo numérico explícitamente\n",
    "df_filtered['week_of_year'] = pd.to_numeric(df_filtered['week_of_year'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Codificar eventos como variables dummy\n",
    "df_filtered = pd.get_dummies(df_filtered, columns=['event_name_1', 'event_type_1'], dummy_na=True)\n",
    "\n",
    "# Rellenar valores NaN generados por shift y rolling\n",
    "df_filtered = df_filtered.fillna(0)\n",
    "\n",
    "# Verificar nuevamente el número de filas después del preprocesamiento\n",
    "available_rows = len(df_filtered)\n",
    "print(f\"Número de filas después de preprocesamiento: {available_rows}\")\n",
    "\n",
    "# Ajustar seq_length dinámicamente si hay pocos datos\n",
    "if available_rows <= 1:\n",
    "    raise ValueError(f\"No hay suficientes datos para procesar. Filas disponibles: {available_rows}.\")\n",
    "elif available_rows - 1 < initial_seq_length:\n",
    "    seq_length = max(1, available_rows - 1)\n",
    "    print(f\"Advertencia: seq_length ajustado a {seq_length} debido a datos insuficientes.\")\n",
    "else:\n",
    "    seq_length = initial_seq_length\n",
    "\n",
    "# Normalizar datos\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features = [col for col in df_filtered.columns if col in ['sales', 'lag_1', 'lag_7', 'lag_14', 'rolling_mean_7', 'rolling_std_7', 'rolling_min_7', 'rolling_max_7', 'day_of_week', 'is_weekend', 'month', 'event', 'snap', 'sell_price', 'price_change', 'week_of_year'] or 'event_name_1' in col or 'event_type_1' in col]\n",
    "# Filtrar características con baja varianza\n",
    "features_to_use = []\n",
    "for feat in features:\n",
    "    if df_filtered[feat].var() > 0.01:  # Umbral de varianza\n",
    "        features_to_use.append(feat)\n",
    "    else:\n",
    "        print(f\"Eliminando característica con baja varianza: {feat}\")\n",
    "\n",
    "# Escalar las características seleccionadas\n",
    "df_filtered[features_to_use] = scaler.fit_transform(df_filtered[features_to_use])\n",
    "\n",
    "# Seleccionar columnas para el modelo\n",
    "data_df = df_filtered[features_to_use]\n",
    "print(\"Dtypes de data_df:\")\n",
    "print(data_df.dtypes)\n",
    "print(f\"Número de características después de filtrar: {len(features_to_use)}\")\n",
    "\n",
    "# Asegurarse de que todos los datos sean numéricos\n",
    "data = data_df.values.astype(np.float32)\n",
    "print(f\"Forma de data: {data.shape}\")\n",
    "\n",
    "# Verificar que data sea bidimensional\n",
    "if len(data.shape) == 1:\n",
    "    data = data.reshape(-1, 1)\n",
    "\n",
    "# Generar secuencias\n",
    "X, y = [], []\n",
    "n_samples = len(data) - seq_length\n",
    "if n_samples > 0:\n",
    "    for i in range(n_samples):\n",
    "        seq = data[i:i + seq_length, :]\n",
    "        X.append(seq)\n",
    "        y.append(data[i + seq_length, 0])\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "else:\n",
    "    raise ValueError(f\"No se pueden generar secuencias. Filas disponibles: {len(data)}, secuencia requerida: {seq_length}.\")\n",
    "\n",
    "print(f\"Forma de X después de generación: {X.shape}\")\n",
    "if len(X.shape) != 3:\n",
    "    raise ValueError(f\"X no tiene 3 dimensiones. Forma actual: {X.shape}.\")\n",
    "\n",
    "# --- Paso 2: Dividir en Entrenamiento, Validación y Prueba ---\n",
    "train_split = int(0.7 * len(X))\n",
    "val_split = int(0.85 * len(X))\n",
    "if train_split == 0 or val_split == train_split:\n",
    "    raise ValueError(f\"No hay suficientes secuencias para dividir. Número de secuencias: {len(X)}\")\n",
    "X_train, X_val, X_test = X[:train_split], X[train_split:val_split], X[val_split:]\n",
    "y_train, y_val, y_test = y[:train_split], y[train_split:val_split], y[val_split:]\n",
    "\n",
    "# --- Paso 3: Modelo LightGBM ---\n",
    "# Convertir X_train, X_val, X_test a DataFrame para mantener nombres de características\n",
    "feature_names = [f'feat_{i}_{j}' for i in range(seq_length) for j in range(len(features_to_use))]\n",
    "X_train_lgb = pd.DataFrame(X_train.reshape((X_train.shape[0], -1)), columns=feature_names)\n",
    "X_val_lgb = pd.DataFrame(X_val.reshape((X_val.shape[0], -1)), columns=feature_names)\n",
    "X_test_lgb = pd.DataFrame(X_test.reshape((X_test.shape[0], -1)), columns=feature_names)\n",
    "\n",
    "lgb_params = {\n",
    "    'num_leaves': [5, 10, 15],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'feature_fraction': [0.9, 1.0],\n",
    "    'bagging_fraction': [0.9, 1.0],\n",
    "    'min_data_in_leaf': [1, 3]\n",
    "}\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42, metric='l2', num_threads=16)\n",
    "grid_search = GridSearchCV(lgb_model, lgb_params, cv=TimeSeriesSplit(n_splits=10), scoring='r2', verbose=1)\n",
    "grid_search.fit(X_train_lgb, y_train)\n",
    "lgb_model = grid_search.best_estimator_\n",
    "print(f\"Mejores parámetros LightGBM: {grid_search.best_params_}\")\n",
    "\n",
    "# --- Paso 4: Predicciones ---\n",
    "# Validación\n",
    "y_val_pred = lgb_model.predict(X_val_lgb)\n",
    "y_val_true = y_val\n",
    "\n",
    "# Prueba\n",
    "y_test_pred = lgb_model.predict(X_test_lgb)\n",
    "y_test_true = y_test\n",
    "\n",
    "# Desnormalizar\n",
    "y_test_2d = y_test.reshape(-1, 1)\n",
    "y_pred_2d = y_test_pred.reshape(-1, 1)\n",
    "zeros_test = np.zeros((y_test_2d.shape[0], len(features_to_use)-1))\n",
    "zeros_pred = np.zeros((y_pred_2d.shape[0], len(features_to_use)-1))\n",
    "y_test_to_transform = np.hstack((y_test_2d, zeros_test))\n",
    "y_pred_to_transform = np.hstack((y_pred_2d, zeros_pred))\n",
    "y_test_original = scaler.inverse_transform(y_test_to_transform)[:, 0]\n",
    "y_pred_original = scaler.inverse_transform(y_pred_to_transform)[:, 0]\n",
    "\n",
    "# Invertir la transformación logarítmica\n",
    "y_test_original = np.expm1(y_test_original)\n",
    "y_pred_original = np.expm1(y_pred_original)\n",
    "\n",
    "# Métricas: R² y NWRMSLE\n",
    "mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "# Calcular NWRMSLE\n",
    "weights = np.ones_like(y_test_original)\n",
    "nwrmsle = np.sqrt(np.sum(weights * (np.log1p(y_pred_original) - np.log1p(y_test_original))**2) / np.sum(weights))\n",
    "print(\"\\nMétricas de Error (LightGBM):\")\n",
    "print(f\"- MSE: {mse:.2f}\")\n",
    "print(f\"- MAE: {mae:.2f}\")\n",
    "print(f\"- R²: {r2:.2f}\")\n",
    "print(f\"- NWRMSLE: {nwrmsle:.2f}\")\n",
    "\n",
    "# Visualización\n",
    "test_dates = df_filtered['date'].iloc[val_split + seq_length:].values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates, y_test_original, label='Ventas Reales', color='blue')\n",
    "plt.plot(test_dates, y_pred_original, label='Predicciones LightGBM', color='orange')\n",
    "plt.title(f'Predicciones vs. Realidad ({item_id}, {store_id})')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Ventas')\n",
    "plt.legend()\n",
    "plt.savefig('predictions_vs_reality_lgb.png')\n",
    "print(\"Gráfico guardado como 'predictions_vs_reality_lgb.png'\")\n",
    "\n",
    "# --- Paso 5: Predicciones Futuras ---\n",
    "X_last_lgb = X_test_lgb.iloc[-1:].copy()\n",
    "predictions = []\n",
    "current_sequence_lgb = X_last_lgb.copy()\n",
    "\n",
    "for _ in range(30):\n",
    "    next_pred = lgb_model.predict(current_sequence_lgb)[0]\n",
    "    predictions.append(next_pred)\n",
    "    current_sequence_lgb = np.roll(current_sequence_lgb, -1, axis=1)\n",
    "    current_sequence_lgb.iloc[0, -1] = next_pred\n",
    "\n",
    "# Desnormalizar predicciones\n",
    "predictions = np.array(predictions).reshape(-1, 1)\n",
    "predictions_to_transform = np.hstack((predictions, np.zeros((predictions.shape[0], len(features_to_use)-1))))\n",
    "predictions_original = scaler.inverse_transform(predictions_to_transform)[:, 0]\n",
    "predictions_original = np.expm1(predictions_original)\n",
    "\n",
    "# Fechas futuras\n",
    "last_date = df_filtered['date'].max()\n",
    "future_dates = [last_date + timedelta(days=i) for i in range(1, 31)]\n",
    "\n",
    "print(\"\\nPredicciones para los próximos 30 días:\")\n",
    "for date, pred in zip(future_dates, predictions_original):\n",
    "    print(f\"Predicción para {date.strftime('%Y-%m-%d')}: {pred:.2f} unidades\")\n",
    "\n",
    "# # Guardar modelo\n",
    "# lgb_model.save_model(os.path.join(models_dir, f'lgb_optimized_{item_id}_{store_id}.txt'))\n",
    "# print(f\"Modelo guardado en {models_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
