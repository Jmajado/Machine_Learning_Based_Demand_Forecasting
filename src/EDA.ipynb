{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploraci√≥n y limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/4g9f6f7d2vj886wxt_jkqkyc0000gn/T/ipykernel_1333/1148754393.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(url_data, sep = ',')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>CategoryName</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Price</th>\n",
       "      <th>Class</th>\n",
       "      <th>ModifyDate</th>\n",
       "      <th>Resistant</th>\n",
       "      <th>IsAllergic</th>\n",
       "      <th>VitalityDays</th>\n",
       "      <th>SalesID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Discount</th>\n",
       "      <th>TotalPrice</th>\n",
       "      <th>Date_x</th>\n",
       "      <th>TransactionNumber</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>MiddleInitial</th>\n",
       "      <th>LastName</th>\n",
       "      <th>CityID</th>\n",
       "      <th>Address</th>\n",
       "      <th>CityName</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>CountryID</th>\n",
       "      <th>CountryName</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>State</th>\n",
       "      <th>pib</th>\n",
       "      <th>rpc</th>\n",
       "      <th>wti</th>\n",
       "      <th>Unemployment Rate</th>\n",
       "      <th>Population_2018</th>\n",
       "      <th>personal_income</th>\n",
       "      <th>Crecimiento (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19077</td>\n",
       "      <td>98726</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DNPDCTGZSONAGKEWLEQ7</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>R</td>\n",
       "      <td>Rangel</td>\n",
       "      <td>6</td>\n",
       "      <td>72 West White First Way</td>\n",
       "      <td>Austin</td>\n",
       "      <td>781</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28628666.0</td>\n",
       "      <td>5.762952e+06</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125395</td>\n",
       "      <td>77870</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RSG6YQO7W8ZG9MUWJTLC</td>\n",
       "      <td>Oscar</td>\n",
       "      <td>B</td>\n",
       "      <td>Young</td>\n",
       "      <td>36</td>\n",
       "      <td>38 First Way</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>72819</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9984072.0</td>\n",
       "      <td>5.713350e+06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136141</td>\n",
       "      <td>91603</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J5U5RBSY3GOYU7LBMGH9</td>\n",
       "      <td>Mary</td>\n",
       "      <td>G</td>\n",
       "      <td>Herrera</td>\n",
       "      <td>50</td>\n",
       "      <td>670 White Oak Way</td>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>83634</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139321</td>\n",
       "      <td>6206</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0DKN419DSBL9Y2BWSDPL</td>\n",
       "      <td>Ebony</td>\n",
       "      <td>P</td>\n",
       "      <td>Hancock</td>\n",
       "      <td>59</td>\n",
       "      <td>45 White New Parkway</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>56647</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2911359.0</td>\n",
       "      <td>1.403880e+06</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188395</td>\n",
       "      <td>19057</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RPPCBUJ4ZXY5JJA4HKK3</td>\n",
       "      <td>Lynn</td>\n",
       "      <td>K</td>\n",
       "      <td>Bonilla</td>\n",
       "      <td>61</td>\n",
       "      <td>39 Nobel Boulevard</td>\n",
       "      <td>Miami</td>\n",
       "      <td>6794</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Florida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21244317.0</td>\n",
       "      <td>1.608971e+07</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CategoryID CategoryName            ProductName   Price Class  \\\n",
       "0           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "1           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "2           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "3           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "4           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "\n",
       "                ModifyDate Resistant IsAllergic  VitalityDays  SalesID  \\\n",
       "0  2017-03-03 09:47:09.310      Weak       True           0.0    19077   \n",
       "1  2017-03-03 09:47:09.310      Weak       True           0.0   125395   \n",
       "2  2017-03-03 09:47:09.310      Weak       True           0.0   136141   \n",
       "3  2017-03-03 09:47:09.310      Weak       True           0.0   139321   \n",
       "4  2017-03-03 09:47:09.310      Weak       True           0.0   188395   \n",
       "\n",
       "   CustomerID  Quantity  ProductID  Discount  TotalPrice Date_x  \\\n",
       "0       98726        25         15       0.1         0.0    NaN   \n",
       "1       77870        20         15       0.0         0.0    NaN   \n",
       "2       91603        24         15       0.0         0.0    NaN   \n",
       "3        6206         2         15       0.0         0.0    NaN   \n",
       "4       19057         5         15       0.0         0.0    NaN   \n",
       "\n",
       "      TransactionNumber FirstName MiddleInitial LastName  CityID  \\\n",
       "0  DNPDCTGZSONAGKEWLEQ7    Thomas             R   Rangel       6   \n",
       "1  RSG6YQO7W8ZG9MUWJTLC     Oscar             B    Young      36   \n",
       "2  J5U5RBSY3GOYU7LBMGH9      Mary             G  Herrera      50   \n",
       "3  0DKN419DSBL9Y2BWSDPL     Ebony             P  Hancock      59   \n",
       "4  RPPCBUJ4ZXY5JJA4HKK3      Lynn             K  Bonilla      61   \n",
       "\n",
       "                   Address    CityName  Zipcode  CountryID    CountryName  \\\n",
       "0  72 West White First Way      Austin      781         32  United States   \n",
       "1             38 First Way     Detroit    72819         32  United States   \n",
       "2        670 White Oak Way  Cincinnati    83634         32  United States   \n",
       "3     45 White New Parkway      Kansas    56647         32  United States   \n",
       "4       39 Nobel Boulevard       Miami     6794         32  United States   \n",
       "\n",
       "  CountryCode     State  pib  rpc  wti  Unemployment Rate  Population_2018  \\\n",
       "0          AR     Texas  NaN  NaN  NaN                NaN       28628666.0   \n",
       "1          AR  Michigan  NaN  NaN  NaN                NaN        9984072.0   \n",
       "2          AR      Ohio  NaN  NaN  NaN                NaN       11676341.0   \n",
       "3          AR    Kansas  NaN  NaN  NaN                NaN        2911359.0   \n",
       "4          AR   Florida  NaN  NaN  NaN                NaN       21244317.0   \n",
       "\n",
       "   personal_income  Crecimiento (%)  \n",
       "0     5.762952e+06             1.18  \n",
       "1     5.713350e+06             0.11  \n",
       "2     6.392306e+06             0.14  \n",
       "3     1.403880e+06             0.09  \n",
       "4     1.608971e+07             1.34  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_data = os.path.join(current_dir, \"../data/raw/Dataframe_Final_Data.csv\")\n",
    "data = pd.read_csv(url_data, sep = ',')\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las dimensiones.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener informaci√≥n sobre tipos de datos y valores no nulos.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay duplicados y eliminarlos si los hubiese.\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar columnas sin relevancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: An√°lisis de variables univariante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An√°lisis sobre variables categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An√°lisis sobre variables num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_variables = data_limpia.select_dtypes(include = ['number']).drop(columns=['Outcome']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "def plot_numericas(data_set, variables_numericas):\n",
    "    \n",
    "    # Crear la figura con 1 columnas y 2 filas por variable.\n",
    "    fig, axis = plt.subplots(len(variables_numericas) * 2, 1, figsize=(8, len(variables_numericas) * 7))\n",
    "\n",
    "    # Definir l√≠mites de los ejes x para cada par de gr√°ficos (histograma y boxplot).\n",
    "    # RECORDAR CAMBIAR LOS NOMBRES Y PONER LOS DE LAS COLUMNAS DE DF CORRESPONDIENTE.\n",
    "    x_limits = {\n",
    "        'age': (0, 100),   # Rango para el histograma y el boxplot de columna1...\n",
    "        'duration': (0, 2000),    \n",
    "        'campaign': (0, 20), \n",
    "        'pdays': (0, 2000),\n",
    "           \n",
    "        # A√±adir m√°s columnas y rangos si es necesario...\n",
    "    }\n",
    "\n",
    "    # Iterar sobre cada columna del DataFrame\n",
    "    for i, col in enumerate(variables_numericas):\n",
    "        index = i * 2\n",
    "        # Histograma en la primera fila\n",
    "        sns.histplot(data = data_set, x = col, kde = True, ax = axis[index])\n",
    "        axis[index].set_title(f'Histogram of {col}')\n",
    "        \n",
    "        # Establecer l√≠mites del eje x para el histograma\n",
    "        if col in x_limits:\n",
    "            axis[index].set_xlim(x_limits[col])  # Asigna el rango de valores personalizado al histograma\n",
    "\n",
    "        # Boxplot en la segunda fila\n",
    "        sns.boxplot(data = data_set, x = col, ax = axis[index + 1])\n",
    "        axis[index + 1].set_title(f'Boxplot of {col}')\n",
    "        \n",
    "        # Establecer l√≠mites del eje x para el boxplot (mismo rango que el histograma)\n",
    "        if col in x_limits:\n",
    "            axis[index + 1].set_xlim(x_limits[col])  # Asigna el mismo rango de valores al boxplot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numericas(data_limpia, numericals_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4: An√°lisis de variables multivariante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An√°lisis num√©rico-num√©rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numericals_variables = data_limpia.select_dtypes(include = ['number']).drop(columns=['Outcome']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "  \n",
    "def plot_numerico_numerico(data_set, variables_numericas):\n",
    "\n",
    "    target = 'Outcome' # Recordar cambiar el target.     \n",
    "    \n",
    "    # Crear una figura con 1 columna y 2 filas por cada variable\n",
    "    fig, axis = plt.subplots(len(variables_numericas) * 2, 1, figsize=(8, (len(variables_numericas) * 5)))\n",
    "\n",
    "    # Crear un diagrama de dispersi√≥n m√∫ltiple                \n",
    "    for i, col in enumerate(variables_numericas):\n",
    "\n",
    "        # Regplot en la primera fila (fila 2 * i)\n",
    "        sns.regplot(ax = axis[i * 2], data = data_set, x = col, y = target)\n",
    "        axis[i * 2].set_title(f'Regplot of {col} vs {target}')\n",
    "        \n",
    "        # Heatmap en la segunda fila.\n",
    "        sns.heatmap(data_set[[col, target]].corr(), annot = True, fmt = \".2f\", ax = axis[i * 2 + 1], cbar = True)\n",
    "        axis[i * 2 + 1].set_title(f'Correlation Heatmap of {col} vs {target}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numerico_numerico(data_limpia, numericals_variables)\n",
    "\n",
    "data_limpia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An√°lisis de posibles relaciones entre variables numericas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An√°lisis categ√≥rico-categ√≥rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinaciones de la clase con varias predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)\n",
    "\n",
    "sns.barplot(ax = axis[0], data = data_limpia, x = \"Outcome\", y = 'Glucose', hue = 'BMI')\n",
    "\n",
    "sns.barplot(ax = axis[1], data = data_limpia, x = \"Outcome\", y = 'BMI', hue = 'BloodPressure').set(ylabel = None)\n",
    "for tick in axis[1].get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An√°lisis de correlaciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matriz_correlacion(data_set):\n",
    "\n",
    "    corr_matrix = data_set.select_dtypes(include = ['number']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot = True, fmt = \".2f\", linewidths = 0.5, cmap = \"coolwarm\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_matriz_correlacion(data_limpia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Una vez analizada la correlaci√≥n, analicemos los dos casos vistos para corroborar la teor√≠a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez analizada la correlaci√≥n, analicemos los dos casos vistos para corroborar la teor√≠a:\n",
    "\n",
    "fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)\n",
    "\n",
    "sns.regplot(ax = axis[0], data = data_limpia, x = \"Outcome\", y = \"Glucose\", scatter_kws={'edgecolor': 'k', 'alpha': 0.6})\n",
    "sns.regplot(ax = axis[1], data = data_limpia, x = \"Age\", y = \"Pregnancies\", scatter_kws={'edgecolor': 'k', 'alpha': 0.6})\n",
    "axis[0].grid(linestyle='--', alpha=0.7)\n",
    "axis[1].grid(linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairpolot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar el pairplot.\n",
    "\n",
    "sns.pairplot(data = data_limpia)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5: Ingenier√≠a de caracter√≠sticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data[\"Embarked\"].fillna(total_data[\"Embarked\"].mode()[0], inplace = True) # RECORDANDO QUE MODE PARA LAS VARIABLES CATEGORICAS\n",
    "\n",
    "total_data[\"Fare\"].fillna(total_data[\"Fare\"].mean(), inplace = True) # Y MEDIA PARA LAS VARIABLES NUMERICAS\n",
    "\n",
    "total_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencia de nuevas caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de valores:\n",
    "\n",
    "# Separar 'x_con_outliers' y 'x_sin_outliers' e 'Y' en train y test. (resultante 6 excels). \n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "# Dividimos el conjunto de datos en muestras de train y test\n",
    "X_con_outliers = data_limpia_con_outliers.drop(\"Outcome\", axis = 1)[num_variables]\n",
    "X_sin_outliers = data_limpia_sin_outliers.drop(\"Outcome\", axis = 1)[num_variables]\n",
    "y = data_limpia_con_outliers[\"Outcome\"]\n",
    "\n",
    "X_train_con_outliers, X_test_con_outliers, y_train, y_test = train_test_split(X_con_outliers, y, test_size = 0.2, random_state = 42)\n",
    "X_train_sin_outliers, X_test_sin_outliers = train_test_split(X_sin_outliers, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers.xlsx\", index = False)\n",
    "X_train_sin_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers.xlsx\", index = False)\n",
    "X_test_con_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers.xlsx\", index = False)\n",
    "X_test_sin_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers.xlsx\", index = False)\n",
    "y_train.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/y_train.xlsx\", index = False)\n",
    "y_test.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/y_test.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizaci√≥n:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "### NORMALIZAMOS EL DATAFRAME CON OUTLIERS Y LO GUARDAMOS\n",
    "normalizador_con_outliers = StandardScaler()\n",
    "normalizador_con_outliers.fit(X_train_con_outliers)   \n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/normalizador_con_outliers.pkl\", \"wb\") as file: # Guardar el Normalizador. \n",
    "  pickle.dump(normalizador_con_outliers, file)\n",
    "\n",
    "X_train_con_outliers_norm = normalizador_con_outliers.transform(X_train_con_outliers)\n",
    "X_train_con_outliers_norm = pd.DataFrame(X_train_con_outliers_norm, index = X_train_con_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_con_outliers_norm = normalizador_con_outliers.transform(X_test_con_outliers)\n",
    "X_test_con_outliers_norm = pd.DataFrame(X_test_con_outliers_norm, index = X_test_con_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers_norm.xlsx\", index = False)\n",
    "X_test_con_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers_norm.xlsx\", index = False)\n",
    "### NORMALIZAMOS EL DATAFRAME SIN OUTLIERS Y LO GUARDAMOS\n",
    "normalizador_sin_outliers = StandardScaler()\n",
    "normalizador_sin_outliers.fit(X_train_sin_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/normalizador_sin_outliers.pkl\", \"wb\") as file: # Guardar el Normalizador. \n",
    "  pickle.dump(normalizador_sin_outliers, file)\n",
    "\n",
    "X_train_sin_outliers_norm = normalizador_sin_outliers.transform(X_train_sin_outliers)\n",
    "X_train_sin_outliers_norm = pd.DataFrame(X_train_sin_outliers_norm, index = X_train_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_sin_outliers_norm = normalizador_sin_outliers.transform(X_test_sin_outliers)\n",
    "X_test_sin_outliers_norm = pd.DataFrame(X_test_sin_outliers_norm, index = X_test_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sin_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers_norm.xlsx\", index = False)\n",
    "X_test_sin_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers_norm.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Escalado M√≠nimo-M√°ximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado M√≠nimo-M√°ximo:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "### ESCALAMOS EL DATAFRAME CON OUTLIERS Y LO GUARDAMOS\n",
    "scaler_con_outliers = MinMaxScaler()\n",
    "scaler_con_outliers.fit(X_train_con_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/scaler_con_outliers.pkl\", \"wb\") as file: # Guardar el Escaler. \n",
    "  pickle.dump(scaler_con_outliers, file)\n",
    "  \n",
    "X_train_con_outliers_scal = scaler_con_outliers.transform(X_train_con_outliers)\n",
    "X_train_con_outliers_scal = pd.DataFrame(X_train_con_outliers_scal, index = X_train_con_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_con_outliers_scal = scaler_con_outliers.transform(X_test_con_outliers)\n",
    "X_test_con_outliers_scal = pd.DataFrame(X_test_con_outliers_scal, index = X_test_con_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers_scal.xlsx\", index = False)\n",
    "X_test_con_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers_scal.xlsx\", index = False)\n",
    "\n",
    "### ESCALAMOS EL DATAFRAME SIN OUTLIERS Y LO GUARDAMOS\n",
    "scaler_sin_outliers = MinMaxScaler()\n",
    "scaler_sin_outliers.fit(X_train_sin_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/scaler_sin_outliers.pkl\", \"wb\") as file: # Guardar el Escaler. \n",
    "  pickle.dump(scaler_sin_outliers, file)\n",
    "\n",
    "X_train_sin_outliers_scal = scaler_sin_outliers.transform(X_train_sin_outliers)\n",
    "X_train_sin_outliers_scal = pd.DataFrame(X_train_sin_outliers_scal, index = X_train_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_sin_outliers_scal = scaler_sin_outliers.transform(X_test_sin_outliers)\n",
    "X_test_sin_outliers_scal = pd.DataFrame(X_test_sin_outliers_scal, index = X_test_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sin_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers_scal.xlsx\", index = False)\n",
    "X_test_sin_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers_scal.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers_scal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "\n",
    "# Con un valor de k = 4 decimos impl√≠citamente que queremos eliminar 1 caracter√≠stica1 del conjunto de datos.\n",
    "\n",
    "selection_model = SelectKBest(f_classif, k = 7)\n",
    "selection_model.fit(X_train_con_outliers, y_train)\n",
    "\n",
    "ix = selection_model.get_support()\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train_con_outliers), columns = X_train_con_outliers.columns.values[ix])\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test_con_outliers), columns = X_test_con_outliers.columns.values[ix])\n",
    "\n",
    "# GUARDO X_train_sel.columns\n",
    "\n",
    "columns_list = X_train_sel.columns.tolist() # Convierte el objeto Index a una lista.tolist()\n",
    "\n",
    "with open(\"feature_selection_k_7.json\", \"w\") as f:\n",
    "  json.dump(columns_list, f)\n",
    "\n",
    "X_train_sel.head()\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sel.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sel_k7.xlsx\", index = False)\n",
    "X_test_sel.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sel_k7.xlsx\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
