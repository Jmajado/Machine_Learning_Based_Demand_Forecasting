{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploraci√≥n y limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>CategoryName</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Price</th>\n",
       "      <th>Class</th>\n",
       "      <th>ModifyDate</th>\n",
       "      <th>Resistant</th>\n",
       "      <th>IsAllergic</th>\n",
       "      <th>VitalityDays</th>\n",
       "      <th>SalesID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Date</th>\n",
       "      <th>TransactionNumber</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>MiddleInitial</th>\n",
       "      <th>LastName</th>\n",
       "      <th>CityID</th>\n",
       "      <th>Address</th>\n",
       "      <th>CityName</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>CountryID</th>\n",
       "      <th>CountryName</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>State</th>\n",
       "      <th>pib</th>\n",
       "      <th>rpc</th>\n",
       "      <th>wti</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>Population_2018</th>\n",
       "      <th>personal_income</th>\n",
       "      <th>Crecimiento_poblacional</th>\n",
       "      <th>Total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12630</td>\n",
       "      <td>24490</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5BSK7H5X44DGRUWEKJEA</td>\n",
       "      <td>Jill</td>\n",
       "      <td>P</td>\n",
       "      <td>Soto</td>\n",
       "      <td>14</td>\n",
       "      <td>31 New Parkway</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>81678</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>1.225389e+14</td>\n",
       "      <td>1.534967e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6695497.0</td>\n",
       "      <td>3.403557e+06</td>\n",
       "      <td>0.56</td>\n",
       "      <td>227.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115884</td>\n",
       "      <td>95026</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3Q0RRIMLEEIMZ4U2G347</td>\n",
       "      <td>Pamela</td>\n",
       "      <td>Z</td>\n",
       "      <td>Estrada</td>\n",
       "      <td>4</td>\n",
       "      <td>949 Milton Drive</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>20641</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>California</td>\n",
       "      <td>9.364726e+14</td>\n",
       "      <td>1.991167e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>4.8</td>\n",
       "      <td>39461588.0</td>\n",
       "      <td>4.156992e+07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>811.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>217388</td>\n",
       "      <td>27676</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>T2ZK8X0HU1KWKPRQ5MUQ</td>\n",
       "      <td>Anita</td>\n",
       "      <td>B</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>10</td>\n",
       "      <td>30 West Milton Way</td>\n",
       "      <td>Toledo</td>\n",
       "      <td>52048</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2.175128e+14</td>\n",
       "      <td>1.570400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>259.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>364759</td>\n",
       "      <td>11630</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>ILPQKU2EBTVNMTN7FQNL</td>\n",
       "      <td>Dustin</td>\n",
       "      <td>B</td>\n",
       "      <td>Coffey</td>\n",
       "      <td>40</td>\n",
       "      <td>904 Oak Parkway</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>51352</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2.175128e+14</td>\n",
       "      <td>1.570400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>77.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>447481</td>\n",
       "      <td>83733</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>TI5RNV3CZM39NW16FG9M</td>\n",
       "      <td>Bridgette</td>\n",
       "      <td>X</td>\n",
       "      <td>Valenzuela</td>\n",
       "      <td>44</td>\n",
       "      <td>52 Rocky Second Drive</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>73999</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>1.203899e+14</td>\n",
       "      <td>1.524400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6771631.0</td>\n",
       "      <td>3.328174e+06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>570.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CategoryID CategoryName            ProductName   Price Class  \\\n",
       "0           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "1           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "2           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "3           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "4           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "\n",
       "                ModifyDate Resistant IsAllergic  VitalityDays  SalesID  \\\n",
       "0  2017-03-03 09:47:09.310      Weak       True           0.0    12630   \n",
       "1  2017-03-03 09:47:09.310      Weak       True           0.0   115884   \n",
       "2  2017-03-03 09:47:09.310      Weak       True           0.0   217388   \n",
       "3  2017-03-03 09:47:09.310      Weak       True           0.0   364759   \n",
       "4  2017-03-03 09:47:09.310      Weak       True           0.0   447481   \n",
       "\n",
       "   CustomerID  Quantity  ProductID  Discount        Date  \\\n",
       "0       24490         7         15       0.0  2018-01-01   \n",
       "1       95026        25         15       0.0  2018-01-01   \n",
       "2       27676         8         15       0.0  2018-01-01   \n",
       "3       11630         3         15       0.2  2018-01-01   \n",
       "4       83733        22         15       0.2  2018-01-01   \n",
       "\n",
       "      TransactionNumber  FirstName MiddleInitial    LastName  CityID  \\\n",
       "0  5BSK7H5X44DGRUWEKJEA       Jill             P        Soto      14   \n",
       "1  3Q0RRIMLEEIMZ4U2G347     Pamela             Z     Estrada       4   \n",
       "2  T2ZK8X0HU1KWKPRQ5MUQ      Anita             B     Sanchez      10   \n",
       "3  ILPQKU2EBTVNMTN7FQNL     Dustin             B      Coffey      40   \n",
       "4  TI5RNV3CZM39NW16FG9M  Bridgette             X  Valenzuela      44   \n",
       "\n",
       "                 Address      CityName  Zipcode  CountryID    CountryName  \\\n",
       "0         31 New Parkway  Indianapolis    81678         32  United States   \n",
       "1       949 Milton Drive       Fremont    20641         32  United States   \n",
       "2     30 West Milton Way        Toledo    52048         32  United States   \n",
       "3        904 Oak Parkway     Cleveland    51352         32  United States   \n",
       "4  52 Rocky Second Drive       Memphis    73999         32  United States   \n",
       "\n",
       "  CountryCode       State           pib           rpc       wti  \\\n",
       "0          AR     Indiana  1.225389e+14  1.534967e+10  4.628095   \n",
       "1          AR  California  9.364726e+14  1.991167e+10  4.628095   \n",
       "2          AR        Ohio  2.175128e+14  1.570400e+10  4.628095   \n",
       "3          AR        Ohio  2.175128e+14  1.570400e+10  4.628095   \n",
       "4          AR   Tennessee  1.203899e+14  1.524400e+10  4.628095   \n",
       "\n",
       "   Unemployment_Rate  Population_2018  personal_income  \\\n",
       "0                3.8        6695497.0     3.403557e+06   \n",
       "1                4.8       39461588.0     4.156992e+07   \n",
       "2                5.2       11676341.0     6.392306e+06   \n",
       "3                5.2       11676341.0     6.392306e+06   \n",
       "4                3.8        6771631.0     3.328174e+06   \n",
       "\n",
       "   Crecimiento_poblacional  Total_price  \n",
       "0                     0.56       227.09  \n",
       "1                     0.26       811.05  \n",
       "2                     0.14       259.54  \n",
       "3                     0.14        77.86  \n",
       "4                     0.94       570.98  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_data = os.path.join(current_dir, \"../data/raw/Dataframe_Final_Data.csv\")\n",
    "data = pd.read_csv(url_data, sep = ',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6690599, 35)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener las dimensiones.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6690599 entries, 0 to 6690598\n",
      "Data columns (total 35 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   CategoryID               int64  \n",
      " 1   CategoryName             object \n",
      " 2   ProductName              object \n",
      " 3   Price                    float64\n",
      " 4   Class                    object \n",
      " 5   ModifyDate               object \n",
      " 6   Resistant                object \n",
      " 7   IsAllergic               object \n",
      " 8   VitalityDays             float64\n",
      " 9   SalesID                  int64  \n",
      " 10  CustomerID               int64  \n",
      " 11  Quantity                 int64  \n",
      " 12  ProductID                int64  \n",
      " 13  Discount                 float64\n",
      " 14  Date                     object \n",
      " 15  TransactionNumber        object \n",
      " 16  FirstName                object \n",
      " 17  MiddleInitial            object \n",
      " 18  LastName                 object \n",
      " 19  CityID                   int64  \n",
      " 20  Address                  object \n",
      " 21  CityName                 object \n",
      " 22  Zipcode                  int64  \n",
      " 23  CountryID                int64  \n",
      " 24  CountryName              object \n",
      " 25  CountryCode              object \n",
      " 26  State                    object \n",
      " 27  pib                      float64\n",
      " 28  rpc                      float64\n",
      " 29  wti                      float64\n",
      " 30  Unemployment_Rate        float64\n",
      " 31  Population_2018          float64\n",
      " 32  personal_income          float64\n",
      " 33  Crecimiento_poblacional  float64\n",
      " 34  Total_price              float64\n",
      "dtypes: float64(11), int64(8), object(16)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "# Obtener informaci√≥n sobre tipos de datos y valores no nulos.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar si hay duplicados y eliminarlos si los hubiese.\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no encontramos duplicados en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar columnas sin relevancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CategoryID', 'ProductName', 'Price', 'Class', 'ModifyDate',\n",
      "       'Resistant', 'IsAllergic', 'VitalityDays', 'Quantity', 'Discount',\n",
      "       'Date', 'CityName', 'CountryName', 'State', 'pib', 'rpc', 'wti',\n",
      "       'Unemployment_Rate', 'Population_2018', 'personal_income',\n",
      "       'Crecimiento_poblacional', 'Total_price'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Price</th>\n",
       "      <th>Class</th>\n",
       "      <th>ModifyDate</th>\n",
       "      <th>Resistant</th>\n",
       "      <th>IsAllergic</th>\n",
       "      <th>VitalityDays</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Date</th>\n",
       "      <th>CityName</th>\n",
       "      <th>CountryName</th>\n",
       "      <th>State</th>\n",
       "      <th>pib</th>\n",
       "      <th>rpc</th>\n",
       "      <th>wti</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>Population_2018</th>\n",
       "      <th>personal_income</th>\n",
       "      <th>Crecimiento_poblacional</th>\n",
       "      <th>Total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>1.225389e+14</td>\n",
       "      <td>1.534967e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6695497.0</td>\n",
       "      <td>3.403557e+06</td>\n",
       "      <td>0.56</td>\n",
       "      <td>227.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>9.364726e+14</td>\n",
       "      <td>1.991167e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>4.8</td>\n",
       "      <td>39461588.0</td>\n",
       "      <td>4.156992e+07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>811.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Toledo</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2.175128e+14</td>\n",
       "      <td>1.570400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>259.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2.175128e+14</td>\n",
       "      <td>1.570400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>77.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>1.203899e+14</td>\n",
       "      <td>1.524400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6771631.0</td>\n",
       "      <td>3.328174e+06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>570.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CategoryID            ProductName   Price Class               ModifyDate  \\\n",
       "0           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "1           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "2           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "3           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "4           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "\n",
       "  Resistant IsAllergic  VitalityDays  Quantity  Discount        Date  \\\n",
       "0      Weak       True           0.0         7       0.0  2018-01-01   \n",
       "1      Weak       True           0.0        25       0.0  2018-01-01   \n",
       "2      Weak       True           0.0         8       0.0  2018-01-01   \n",
       "3      Weak       True           0.0         3       0.2  2018-01-01   \n",
       "4      Weak       True           0.0        22       0.2  2018-01-01   \n",
       "\n",
       "       CityName    CountryName       State           pib           rpc  \\\n",
       "0  Indianapolis  United States     Indiana  1.225389e+14  1.534967e+10   \n",
       "1       Fremont  United States  California  9.364726e+14  1.991167e+10   \n",
       "2        Toledo  United States        Ohio  2.175128e+14  1.570400e+10   \n",
       "3     Cleveland  United States        Ohio  2.175128e+14  1.570400e+10   \n",
       "4       Memphis  United States   Tennessee  1.203899e+14  1.524400e+10   \n",
       "\n",
       "        wti  Unemployment_Rate  Population_2018  personal_income  \\\n",
       "0  4.628095                3.8        6695497.0     3.403557e+06   \n",
       "1  4.628095                4.8       39461588.0     4.156992e+07   \n",
       "2  4.628095                5.2       11676341.0     6.392306e+06   \n",
       "3  4.628095                5.2       11676341.0     6.392306e+06   \n",
       "4  4.628095                3.8        6771631.0     3.328174e+06   \n",
       "\n",
       "   Crecimiento_poblacional  Total_price  \n",
       "0                     0.56       227.09  \n",
       "1                     0.26       811.05  \n",
       "2                     0.14       259.54  \n",
       "3                     0.14        77.86  \n",
       "4                     0.94       570.98  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns = ['TransactionNumber', 'CategoryName', 'SalesID', 'CountryCode', 'CustomerID', 'ProductID', 'CityID', 'CountryID', 'FirstName', 'MiddleInitial', 'LastName', 'Address', 'Zipcode'], inplace=True)\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columnas que DESCARTAR y por qu√©:\n",
    "\n",
    "`TransactionNumber`: \n",
    "Es simplemente un identificador √∫nico de cada transacci√≥n, no aporta informaci√≥n √∫til para predecir demanda.\n",
    "\n",
    "\n",
    "`SalesID`, `CustomerID`, `ProductID`, `CityID`, `CountryID`:\n",
    "Son IDs arbitrarios que no tienen significado en s√≠ mismos y pueden confundir al modelo.\n",
    "\n",
    "`CategoryName`, `CountryCode`: Tenemos la columna CategoryID  y Country NaME la cual serian estas mismas columna ya categorizadas.\n",
    "\n",
    "\n",
    "`FirstName`, `MiddleInitial`, `LastName`:\n",
    "Son nombres personales; no tienen valor predictivo y pueden introducir sesgo o problemas de privacidad.\n",
    "\n",
    "\n",
    "`Address`, `Zipcode`:\n",
    "Pueden contener informaci√≥n √∫til si se convierten en variables regionales o socioecon√≥micas, pero en bruto son de muy alta cardinalidad y dif√≠ciles de usar directamente. Mejor descartarlas por ahora, a menos que tengamos una forma clara de agruparlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columnas que pueden ser de utilidad pero que hay que modificar:\n",
    "\n",
    "`ModifyDate`, `Date`:\n",
    "\n",
    "En su forma cruda no son √∫tiles, pero podemos extraer de ellas variables como:\n",
    "D√≠a de la semana\n",
    "Mes\n",
    "¬øEs fin de semana o no?\n",
    "¬øEs feriado o no?\n",
    "Entonces: descartarlas como strings, pero sacar variables derivadas antes.\n",
    "\n",
    "`ProductName`, `CountryName`, `CityName`, `State`:\n",
    "\n",
    "Considerar agruparlas o usar solo las m√°s frecuentes. (Factorizarlas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columnas √öTILES para predecir la demanda:\n",
    "\n",
    "`CategoryID`, `Class` ‚Üí tipo de producto.\n",
    "\n",
    "`Resistant`, `IsAllergic`, `VitalityDays` ‚Üí caracter√≠sticas del producto.\n",
    "\n",
    "`Price`, `Discount`, `Total_price` ‚Üí precio y descuentos.\n",
    "\n",
    "`pib`, `rpc` (renta per c√°pita), `wti` (petr√≥leo), `Unemployment_Rate`, `Population_2018`, `personal_income`, `Crecimiento_poblacional` ‚Üí variables macroecon√≥micas que podr√≠an correlacionar con la demanda.\n",
    "\n",
    "Variables derivadas del tiempo (`Date`, `ModifyDate`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: An√°lisis de variables univariante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An√°lisis sobre variables categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An√°lisis sobre variables num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_variables = data_limpia.select_dtypes(include = ['number']).drop(columns=['Outcome']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "def plot_numericas(data_set, variables_numericas):\n",
    "    \n",
    "    # Crear la figura con 1 columnas y 2 filas por variable.\n",
    "    fig, axis = plt.subplots(len(variables_numericas) * 2, 1, figsize=(8, len(variables_numericas) * 7))\n",
    "\n",
    "    # Definir l√≠mites de los ejes x para cada par de gr√°ficos (histograma y boxplot).\n",
    "    # RECORDAR CAMBIAR LOS NOMBRES Y PONER LOS DE LAS COLUMNAS DE DF CORRESPONDIENTE.\n",
    "    x_limits = {\n",
    "        'age': (0, 100),   # Rango para el histograma y el boxplot de columna1...\n",
    "        'duration': (0, 2000),    \n",
    "        'campaign': (0, 20), \n",
    "        'pdays': (0, 2000),\n",
    "           \n",
    "        # A√±adir m√°s columnas y rangos si es necesario...\n",
    "    }\n",
    "\n",
    "    # Iterar sobre cada columna del DataFrame\n",
    "    for i, col in enumerate(variables_numericas):\n",
    "        index = i * 2\n",
    "        # Histograma en la primera fila\n",
    "        sns.histplot(data = data_set, x = col, kde = True, ax = axis[index])\n",
    "        axis[index].set_title(f'Histogram of {col}')\n",
    "        \n",
    "        # Establecer l√≠mites del eje x para el histograma\n",
    "        if col in x_limits:\n",
    "            axis[index].set_xlim(x_limits[col])  # Asigna el rango de valores personalizado al histograma\n",
    "\n",
    "        # Boxplot en la segunda fila\n",
    "        sns.boxplot(data = data_set, x = col, ax = axis[index + 1])\n",
    "        axis[index + 1].set_title(f'Boxplot of {col}')\n",
    "        \n",
    "        # Establecer l√≠mites del eje x para el boxplot (mismo rango que el histograma)\n",
    "        if col in x_limits:\n",
    "            axis[index + 1].set_xlim(x_limits[col])  # Asigna el mismo rango de valores al boxplot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numericas(data_limpia, numericals_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4: An√°lisis de variables multivariante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An√°lisis num√©rico-num√©rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numericals_variables = data_limpia.select_dtypes(include = ['number']).drop(columns=['Outcome']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "  \n",
    "def plot_numerico_numerico(data_set, variables_numericas):\n",
    "\n",
    "    target = 'Outcome' # Recordar cambiar el target.     \n",
    "    \n",
    "    # Crear una figura con 1 columna y 2 filas por cada variable\n",
    "    fig, axis = plt.subplots(len(variables_numericas) * 2, 1, figsize=(8, (len(variables_numericas) * 5)))\n",
    "\n",
    "    # Crear un diagrama de dispersi√≥n m√∫ltiple                \n",
    "    for i, col in enumerate(variables_numericas):\n",
    "\n",
    "        # Regplot en la primera fila (fila 2 * i)\n",
    "        sns.regplot(ax = axis[i * 2], data = data_set, x = col, y = target)\n",
    "        axis[i * 2].set_title(f'Regplot of {col} vs {target}')\n",
    "        \n",
    "        # Heatmap en la segunda fila.\n",
    "        sns.heatmap(data_set[[col, target]].corr(), annot = True, fmt = \".2f\", ax = axis[i * 2 + 1], cbar = True)\n",
    "        axis[i * 2 + 1].set_title(f'Correlation Heatmap of {col} vs {target}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numerico_numerico(data_limpia, numericals_variables)\n",
    "\n",
    "data_limpia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An√°lisis de posibles relaciones entre variables numericas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An√°lisis categ√≥rico-categ√≥rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinaciones de la clase con varias predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)\n",
    "\n",
    "sns.barplot(ax = axis[0], data = data_limpia, x = \"Outcome\", y = 'Glucose', hue = 'BMI')\n",
    "\n",
    "sns.barplot(ax = axis[1], data = data_limpia, x = \"Outcome\", y = 'BMI', hue = 'BloodPressure').set(ylabel = None)\n",
    "for tick in axis[1].get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An√°lisis de correlaciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matriz_correlacion(data_set):\n",
    "\n",
    "    corr_matrix = data_set.select_dtypes(include = ['number']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot = True, fmt = \".2f\", linewidths = 0.5, cmap = \"coolwarm\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_matriz_correlacion(data_limpia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Una vez analizada la correlaci√≥n, analicemos los dos casos vistos para corroborar la teor√≠a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez analizada la correlaci√≥n, analicemos los dos casos vistos para corroborar la teor√≠a:\n",
    "\n",
    "fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)\n",
    "\n",
    "sns.regplot(ax = axis[0], data = data_limpia, x = \"Outcome\", y = \"Glucose\", scatter_kws={'edgecolor': 'k', 'alpha': 0.6})\n",
    "sns.regplot(ax = axis[1], data = data_limpia, x = \"Age\", y = \"Pregnancies\", scatter_kws={'edgecolor': 'k', 'alpha': 0.6})\n",
    "axis[0].grid(linestyle='--', alpha=0.7)\n",
    "axis[1].grid(linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairpolot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar el pairplot.\n",
    "\n",
    "sns.pairplot(data = data_limpia)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5: Ingenier√≠a de caracter√≠sticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data[\"Embarked\"].fillna(total_data[\"Embarked\"].mode()[0], inplace = True) # RECORDANDO QUE MODE PARA LAS VARIABLES CATEGORICAS\n",
    "\n",
    "total_data[\"Fare\"].fillna(total_data[\"Fare\"].mean(), inplace = True) # Y MEDIA PARA LAS VARIABLES NUMERICAS\n",
    "\n",
    "total_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencia de nuevas caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de valores:\n",
    "\n",
    "# Separar 'x_con_outliers' y 'x_sin_outliers' e 'Y' en train y test. (resultante 6 excels). \n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "# Dividimos el conjunto de datos en muestras de train y test\n",
    "X_con_outliers = data_limpia_con_outliers.drop(\"Outcome\", axis = 1)[num_variables]\n",
    "X_sin_outliers = data_limpia_sin_outliers.drop(\"Outcome\", axis = 1)[num_variables]\n",
    "y = data_limpia_con_outliers[\"Outcome\"]\n",
    "\n",
    "X_train_con_outliers, X_test_con_outliers, y_train, y_test = train_test_split(X_con_outliers, y, test_size = 0.2, random_state = 42)\n",
    "X_train_sin_outliers, X_test_sin_outliers = train_test_split(X_sin_outliers, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers.xlsx\", index = False)\n",
    "X_train_sin_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers.xlsx\", index = False)\n",
    "X_test_con_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers.xlsx\", index = False)\n",
    "X_test_sin_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers.xlsx\", index = False)\n",
    "y_train.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/y_train.xlsx\", index = False)\n",
    "y_test.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/y_test.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizaci√≥n:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "### NORMALIZAMOS EL DATAFRAME CON OUTLIERS Y LO GUARDAMOS\n",
    "normalizador_con_outliers = StandardScaler()\n",
    "normalizador_con_outliers.fit(X_train_con_outliers)   \n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/normalizador_con_outliers.pkl\", \"wb\") as file: # Guardar el Normalizador. \n",
    "  pickle.dump(normalizador_con_outliers, file)\n",
    "\n",
    "X_train_con_outliers_norm = normalizador_con_outliers.transform(X_train_con_outliers)\n",
    "X_train_con_outliers_norm = pd.DataFrame(X_train_con_outliers_norm, index = X_train_con_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_con_outliers_norm = normalizador_con_outliers.transform(X_test_con_outliers)\n",
    "X_test_con_outliers_norm = pd.DataFrame(X_test_con_outliers_norm, index = X_test_con_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers_norm.xlsx\", index = False)\n",
    "X_test_con_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers_norm.xlsx\", index = False)\n",
    "### NORMALIZAMOS EL DATAFRAME SIN OUTLIERS Y LO GUARDAMOS\n",
    "normalizador_sin_outliers = StandardScaler()\n",
    "normalizador_sin_outliers.fit(X_train_sin_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/normalizador_sin_outliers.pkl\", \"wb\") as file: # Guardar el Normalizador. \n",
    "  pickle.dump(normalizador_sin_outliers, file)\n",
    "\n",
    "X_train_sin_outliers_norm = normalizador_sin_outliers.transform(X_train_sin_outliers)\n",
    "X_train_sin_outliers_norm = pd.DataFrame(X_train_sin_outliers_norm, index = X_train_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_sin_outliers_norm = normalizador_sin_outliers.transform(X_test_sin_outliers)\n",
    "X_test_sin_outliers_norm = pd.DataFrame(X_test_sin_outliers_norm, index = X_test_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sin_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers_norm.xlsx\", index = False)\n",
    "X_test_sin_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers_norm.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Escalado M√≠nimo-M√°ximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado M√≠nimo-M√°ximo:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "### ESCALAMOS EL DATAFRAME CON OUTLIERS Y LO GUARDAMOS\n",
    "scaler_con_outliers = MinMaxScaler()\n",
    "scaler_con_outliers.fit(X_train_con_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/scaler_con_outliers.pkl\", \"wb\") as file: # Guardar el Escaler. \n",
    "  pickle.dump(scaler_con_outliers, file)\n",
    "  \n",
    "X_train_con_outliers_scal = scaler_con_outliers.transform(X_train_con_outliers)\n",
    "X_train_con_outliers_scal = pd.DataFrame(X_train_con_outliers_scal, index = X_train_con_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_con_outliers_scal = scaler_con_outliers.transform(X_test_con_outliers)\n",
    "X_test_con_outliers_scal = pd.DataFrame(X_test_con_outliers_scal, index = X_test_con_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers_scal.xlsx\", index = False)\n",
    "X_test_con_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers_scal.xlsx\", index = False)\n",
    "\n",
    "### ESCALAMOS EL DATAFRAME SIN OUTLIERS Y LO GUARDAMOS\n",
    "scaler_sin_outliers = MinMaxScaler()\n",
    "scaler_sin_outliers.fit(X_train_sin_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/scaler_sin_outliers.pkl\", \"wb\") as file: # Guardar el Escaler. \n",
    "  pickle.dump(scaler_sin_outliers, file)\n",
    "\n",
    "X_train_sin_outliers_scal = scaler_sin_outliers.transform(X_train_sin_outliers)\n",
    "X_train_sin_outliers_scal = pd.DataFrame(X_train_sin_outliers_scal, index = X_train_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_sin_outliers_scal = scaler_sin_outliers.transform(X_test_sin_outliers)\n",
    "X_test_sin_outliers_scal = pd.DataFrame(X_test_sin_outliers_scal, index = X_test_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sin_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers_scal.xlsx\", index = False)\n",
    "X_test_sin_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers_scal.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers_scal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "\n",
    "# Con un valor de k = 4 decimos impl√≠citamente que queremos eliminar 1 caracter√≠stica1 del conjunto de datos.\n",
    "\n",
    "selection_model = SelectKBest(f_classif, k = 7)\n",
    "selection_model.fit(X_train_con_outliers, y_train)\n",
    "\n",
    "ix = selection_model.get_support()\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train_con_outliers), columns = X_train_con_outliers.columns.values[ix])\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test_con_outliers), columns = X_test_con_outliers.columns.values[ix])\n",
    "\n",
    "# GUARDO X_train_sel.columns\n",
    "\n",
    "columns_list = X_train_sel.columns.tolist() # Convierte el objeto Index a una lista.tolist()\n",
    "\n",
    "with open(\"feature_selection_k_7.json\", \"w\") as f:\n",
    "  json.dump(columns_list, f)\n",
    "\n",
    "X_train_sel.head()\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sel.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sel_k7.xlsx\", index = False)\n",
    "X_test_sel.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sel_k7.xlsx\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
